1.) working with minibatches of size 16 is around 50 seconds for each epoch where as minibatches of size 32 is around 70seconds
2.)Mobile Net Accuracy: 0.4
Precision: 0.9929278642149929
recall: 1.0
f1-score: 0.9964513839602556
Running Time: 658.05 seconds
for batch size 32,epoch 15
by augumenting the test data
x_test = np.concatenate([
    darkening(x_test[:2000], 1.5),
    shift_left(x_test[2000:4000], 40),
    flip_vertical_inplace(x_test[4000:6000],),
    flip_horizontal_inplace(x_test[6000:8000]),
    shift_right(x_test[8000:],40)
])

WARNING:tensorflow:From C:\Users\hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

(32000, 28, 28, 1)
(32000,)
(10000, 28, 28, 1)
(10000,)
WARNING:tensorflow:From C:\Users\hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.

WARNING:tensorflow:From C:\Users\hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\layers\normalization\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.

Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 28, 28, 1)]       0         
                                                                 
 conv2d (Conv2D)             (None, 14, 14, 32)        320       
                                                                 
 batch_normalization (Batch  (None, 14, 14, 32)        128       
 Normalization)                                                  
                                                                 
 re_lu (ReLU)                (None, 14, 14, 32)        0         
                                                                 
 depthwise_conv2d (Depthwis  (None, 14, 14, 32)        320       
 eConv2D)                                                        
                                                                 
 batch_normalization_1 (Bat  (None, 14, 14, 32)        128       
 chNormalization)                                                
                                                                 
 re_lu_1 (ReLU)              (None, 14, 14, 32)        0         
                                                                 
 conv2d_1 (Conv2D)           (None, 14, 14, 64)        2112      
                                                                 
 batch_normalization_2 (Bat  (None, 14, 14, 64)        256       
 chNormalization)                                                
                                                                 
 re_lu_2 (ReLU)              (None, 14, 14, 64)        0         
                                                                 
 depthwise_conv2d_1 (Depthw  (None, 7, 7, 64)          640       
 iseConv2D)                                                      
                                                                 
 batch_normalization_3 (Bat  (None, 7, 7, 64)          256       
 chNormalization)                                                
                                                                 
 re_lu_3 (ReLU)              (None, 7, 7, 64)          0         
                                                                 
 conv2d_2 (Conv2D)           (None, 7, 7, 128)         8320      
                                                                 
 batch_normalization_4 (Bat  (None, 7, 7, 128)         512       
 chNormalization)                                                
                                                                 
 re_lu_4 (ReLU)              (None, 7, 7, 128)         0         
                                                                 
 depthwise_conv2d_2 (Depthw  (None, 7, 7, 128)         1280      
 iseConv2D)                                                      
                                                                 
 batch_normalization_5 (Bat  (None, 7, 7, 128)         512       
 chNormalization)                                                
                                                                 
 re_lu_5 (ReLU)              (None, 7, 7, 128)         0         
                                                                 
 conv2d_3 (Conv2D)           (None, 7, 7, 128)         16512     
                                                                 
 batch_normalization_6 (Bat  (None, 7, 7, 128)         512       
 chNormalization)                                                
                                                                 
 re_lu_6 (ReLU)              (None, 7, 7, 128)         0         
                                                                 
 depthwise_conv2d_3 (Depthw  (None, 4, 4, 128)         1280      
 iseConv2D)                                                      
                                                                 
 batch_normalization_7 (Bat  (None, 4, 4, 128)         512       
 chNormalization)                                                
                                                                 
 re_lu_7 (ReLU)              (None, 4, 4, 128)         0         
                                                                 
 conv2d_4 (Conv2D)           (None, 4, 4, 256)         33024     
                                                                 
 batch_normalization_8 (Bat  (None, 4, 4, 256)         1024      
 chNormalization)                                                
                                                                 
 re_lu_8 (ReLU)              (None, 4, 4, 256)         0         
                                                                 
 depthwise_conv2d_4 (Depthw  (None, 4, 4, 256)         2560      
 iseConv2D)                                                      
                                                                 
 batch_normalization_9 (Bat  (None, 4, 4, 256)         1024      
 chNormalization)                                                
                                                                 
 re_lu_9 (ReLU)              (None, 4, 4, 256)         0         
                                                                 
 conv2d_5 (Conv2D)           (None, 4, 4, 256)         65792     
                                                                 
 batch_normalization_10 (Ba  (None, 4, 4, 256)         1024      
 tchNormalization)                                               
                                                                 
 re_lu_10 (ReLU)             (None, 4, 4, 256)         0         
                                                                 
 depthwise_conv2d_5 (Depthw  (None, 2, 2, 256)         2560      
 iseConv2D)                                                      
                                                                 
 batch_normalization_11 (Ba  (None, 2, 2, 256)         1024      
 tchNormalization)                                               
                                                                 
 re_lu_11 (ReLU)             (None, 2, 2, 256)         0         
                                                                 
 conv2d_6 (Conv2D)           (None, 2, 2, 512)         131584    
                                                                 
 batch_normalization_12 (Ba  (None, 2, 2, 512)         2048      
 tchNormalization)                                               
                                                                 
 re_lu_12 (ReLU)             (None, 2, 2, 512)         0         
                                                                 
 global_average_pooling2d (  (None, 512)               0         
 GlobalAveragePooling2D)                                         
                                                                 
 dense (Dense)               (None, 1024)              525312    
                                                                 
 dropout (Dropout)           (None, 1024)              0         
                                                                 
 dense_1 (Dense)             (None, 10)                10250     
                                                                 
=================================================================
Total params: 810826 (3.09 MB)
Trainable params: 806346 (3.08 MB)
Non-trainable params: 4480 (17.50 KB)
_________________________________________________________________
WARNING:tensorflow:From C:\Users\hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\optimizers\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

Epoch 1/15
WARNING:tensorflow:From C:\Users\hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\utils\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.

WARNING:tensorflow:From C:\Users\hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\engine\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.

1000/1000 [==============================] - 45s 39ms/step - loss: 0.2930 - accuracy: 0.9066
Epoch 2/15
1000/1000 [==============================] - 39s 39ms/step - loss: 0.0989 - accuracy: 0.9713
Epoch 3/15
1000/1000 [==============================] - 43s 43ms/step - loss: 0.0816 - accuracy: 0.9764
Epoch 4/15
1000/1000 [==============================] - 42s 42ms/step - loss: 0.0690 - accuracy: 0.9803
Epoch 5/15
1000/1000 [==============================] - 43s 44ms/step - loss: 0.0572 - accuracy: 0.9833
Epoch 6/15
1000/1000 [==============================] - 45s 45ms/step - loss: 0.0534 - accuracy: 0.9838
Epoch 7/15
1000/1000 [==============================] - 43s 43ms/step - loss: 0.0458 - accuracy: 0.9865
Epoch 8/15
1000/1000 [==============================] - 44s 44ms/step - loss: 0.0401 - accuracy: 0.9889
Epoch 9/15
1000/1000 [==============================] - 44s 44ms/step - loss: 0.0393 - accuracy: 0.9895
Epoch 10/15
1000/1000 [==============================] - 43s 43ms/step - loss: 0.0328 - accuracy: 0.9904
Epoch 11/15
1000/1000 [==============================] - 44s 44ms/step - loss: 0.0319 - accuracy: 0.9912
Epoch 12/15
1000/1000 [==============================] - 44s 44ms/step - loss: 0.0375 - accuracy: 0.9894
Epoch 13/15
1000/1000 [==============================] - 44s 44ms/step - loss: 0.0261 - accuracy: 0.9929
Epoch 14/15
1000/1000 [==============================] - 44s 44ms/step - loss: 0.0277 - accuracy: 0.9922
Epoch 15/15
1000/1000 [==============================] - 44s 44ms/step - loss: 0.0215 - accuracy: 0.9946
313/313 [==============================] - 5s 14ms/step
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 28, 28, 1)]       0         
                                                                 
 conv2d_7 (Conv2D)           (None, 28, 28, 24)        240       
                                                                 
 batch_normalization_13 (Ba  (None, 28, 28, 24)        96        
 tchNormalization)                                               
                                                                 
 re_lu_13 (ReLU)             (None, 28, 28, 24)        0         
                                                                 
 depthwise_conv2d_6 (Depthw  (None, 28, 28, 24)        240       
 iseConv2D)                                                      
                                                                 
 batch_normalization_14 (Ba  (None, 28, 28, 24)        96        
 tchNormalization)                                               
                                                                 
 re_lu_14 (ReLU)             (None, 28, 28, 24)        0         
                                                                 
 conv2d_8 (Conv2D)           (None, 28, 28, 24)        600       
                                                                 
 batch_normalization_15 (Ba  (None, 28, 28, 24)        96        
 tchNormalization)                                               
                                                                 
 re_lu_15 (ReLU)             (None, 28, 28, 24)        0         
                                                                 
 depthwise_conv2d_7 (Depthw  (None, 28, 28, 24)        240       
 iseConv2D)                                                      
                                                                 
 batch_normalization_16 (Ba  (None, 28, 28, 24)        96        
 tchNormalization)                                               
                                                                 
 re_lu_16 (ReLU)             (None, 28, 28, 24)        0         
                                                                 
 conv2d_9 (Conv2D)           (None, 28, 28, 24)        600       
                                                                 
 batch_normalization_17 (Ba  (None, 28, 28, 24)        96        
 tchNormalization)                                               
                                                                 
 re_lu_17 (ReLU)             (None, 28, 28, 24)        0         
                                                                 
 depthwise_conv2d_8 (Depthw  (None, 28, 28, 24)        240       
 iseConv2D)                                                      
                                                                 
 batch_normalization_18 (Ba  (None, 28, 28, 24)        96        
 tchNormalization)                                               
                                                                 
 re_lu_18 (ReLU)             (None, 28, 28, 24)        0         
                                                                 
 conv2d_10 (Conv2D)          (None, 28, 28, 24)        600       
                                                                 
 batch_normalization_19 (Ba  (None, 28, 28, 24)        96        
 tchNormalization)                                               
                                                                 
 re_lu_19 (ReLU)             (None, 28, 28, 24)        0         
                                                                 
 tf.reshape (TFOpLambda)     (None, 28, 28, 3, 8)      0         
                                                                 
 tf.compat.v1.transpose (TF  (None, 28, 28, 8, 3)      0         
 OpLambda)                                                       
                                                                 
 tf.reshape_1 (TFOpLambda)   (None, 28, 28, 24)        0         
                                                                 
 global_average_pooling2d_1  (None, 24)                0         
  (GlobalAveragePooling2D)                                       
                                                                 
 dense_2 (Dense)             (None, 1024)              25600     
                                                                 
 dropout_1 (Dropout)         (None, 1024)              0         
                                                                 
 dense_3 (Dense)             (None, 10)                10250     
                                                                 
=================================================================
Total params: 39282 (153.45 KB)
Trainable params: 38946 (152.13 KB)
Non-trainable params: 336 (1.31 KB)
_________________________________________________________________
Epoch 1/15
1000/1000 [==============================] - 45s 42ms/step - loss: 0.9090 - accuracy: 0.6792
Epoch 2/15
1000/1000 [==============================] - 42s 42ms/step - loss: 0.3559 - accuracy: 0.8898
Epoch 3/15
1000/1000 [==============================] - 42s 42ms/step - loss: 0.2279 - accuracy: 0.9317
Epoch 4/15
1000/1000 [==============================] - 43s 43ms/step - loss: 0.1962 - accuracy: 0.9407
Epoch 5/15
1000/1000 [==============================] - 42s 42ms/step - loss: 0.1724 - accuracy: 0.9473
Epoch 6/15
1000/1000 [==============================] - 42s 42ms/step - loss: 0.1573 - accuracy: 0.9511
Epoch 7/15
1000/1000 [==============================] - 42s 42ms/step - loss: 0.1425 - accuracy: 0.9561
Epoch 8/15
1000/1000 [==============================] - 42s 42ms/step - loss: 0.1283 - accuracy: 0.9601
Epoch 9/15
1000/1000 [==============================] - 42s 42ms/step - loss: 0.1245 - accuracy: 0.9613
Epoch 10/15
1000/1000 [==============================] - 42s 42ms/step - loss: 0.1137 - accuracy: 0.9646
Epoch 11/15
1000/1000 [==============================] - 43s 43ms/step - loss: 0.1101 - accuracy: 0.9670
Epoch 12/15
1000/1000 [==============================] - 38s 38ms/step - loss: 0.1018 - accuracy: 0.9673
Epoch 13/15
1000/1000 [==============================] - 35s 35ms/step - loss: 0.1028 - accuracy: 0.9677
Epoch 14/15
1000/1000 [==============================] - 35s 35ms/step - loss: 0.0964 - accuracy: 0.9703
Epoch 15/15
1000/1000 [==============================] - 35s 35ms/step - loss: 0.0928 - accuracy: 0.9716
313/313 [==============================] - 4s 12ms/step
Accuracy:
MobileNet-0.9877 ShuffleNet-0.9617
Precision:
MobileNet-1.0 ShuffleNet-1.0
Recall:
MobileNet-1.0 ShuffleNet-1.0
F1-score:
MobileNet-1.0 SuffleNet-1.0
Average:
MobileNet-0.9969250000000001 ShuffleNet-0.990425
Running time:
 MobileNet-655.94 seconds ShuffleNet-612.67 seconds
Certainly, let's evaluate the models based on the provided results:

1. **Accuracy:**
   - MobileNet: 98.77%
   - ShuffleNet: 96.17%

   **Observation:**
   MobileNet outperforms ShuffleNet in terms of accuracy.

2. **Precision:**
   - MobileNet: 100%
   - ShuffleNet: 100%

   **Observation:**
   Both models achieve perfect precision, indicating no false positives.

3. **Recall:**
   - MobileNet: 100%
   - ShuffleNet: 100%

   **Observation:**
   Both models achieve perfect recall, indicating no false negatives.

4. **F1-score:**
   - MobileNet: 100%
   - ShuffleNet: 100%

   **Observation:**
   Both models achieve a perfect F1-score, indicating a perfect balance between precision and recall.

5. **Average:**
   - MobileNet: 99.69%
   - ShuffleNet: 99.04%

   **Observation:**
   MobileNet has a higher average metric, indicating better overall performance across the evaluated metrics.

6. **Running Time:**
   - MobileNet: 655.94 seconds
   - ShuffleNet: 612.67 seconds

   **Observation:**
   ShuffleNet took less time to train and test compared to MobileNet.

**Evaluation:**
- **Accuracy and Precision:**
  - Both models achieved perfect accuracy and precision, indicating robust performance without misclassifications.
- **Recall and F1-score:**
  - Perfect recall and F1-score suggest that both models correctly identified all positive instances without any false negatives.
- **Average Metric:**
  - MobileNet has a higher average metric, indicating better overall performance across multiple metrics.
- **Running Time:**
  - ShuffleNet performed slightly better in terms of running time, being faster than MobileNet.

**Recommendations:**
- The choice between MobileNet and ShuffleNet depends on the specific requirements and constraints, considering factors like accuracy, computational efficiency, and time constraints.
- Continue monitoring learning curves and consider fine-tuning hyperparameters for further optimization.

To evaluate whether the models are overfitting or underfitting, you can look at the performance metrics and the running time. Here are some considerations:

1. **Accuracy:**
   - High accuracy on both training and testing sets generally indicates a well-fit model.
   - If there is a significant gap between training and testing accuracy, it might suggest overfitting.

2. **Precision, Recall, and F1-score:**
   - Precision measures the accuracy of positive predictions, recall measures the ability to find all positive instances, and F1-score is the harmonic mean of precision and recall.
   - High precision, recall, and F1-score on both training and testing sets suggest a well-fit model.

3. **Average Metrics:**
   - Average metrics provide an overall summary. Comparing training and testing averages can give insights into generalization.

4. **Running Time:**
   - Overfitting might occur if a model is too complex and learns noise in the training data.
   - Considerable differences in running time might suggest different levels of model complexity.

Based on the provided metrics:

- Both MobileNet and ShuffleNet seem to perform well, with high accuracy, precision, recall, and F1-score on the testing set.
- The averages for both models are reasonably close on the testing set, indicating good generalization.

However, you need to consider the context and specific requirements of your project. If the training time of MobileNet is significantly longer than ShuffleNet, you might want to weigh the trade-off between model complexity and computational efficiency.

In conclusion, based on the provided results, the models do not appear to suffer from significant overfitting or underfitting. The high metrics on the testing set and the relatively close averages between training and testing suggest good model performance. The difference in running time may be due to the inherent complexity of the models, but it does not necessarily indicate overfitting or underfitting.
